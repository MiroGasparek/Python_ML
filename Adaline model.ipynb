{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive linear neuron (Adaline) model\n",
    "The difference betwwen the perceptron and the Adaline neuron is that the activation function of the Adaline neuron is a linear function rather than step function as in the case of the perceptron.\n",
    "\n",
    "The activation function $\\phi(\\mathbf{w}^{T} \\mathbf{x})$ of the Adaline neuron is a linear function: <br>\n",
    "$\\phi(\\mathbf{w}^{T} \\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$\n",
    "\n",
    "## The objective function\n",
    "How do we update the weights? We should probably update them in such way that decreases the difference between the observed output and the predicted output. However, we probably are interested in minimization of some *norm* of this difference. <br>\n",
    "\n",
    "The typical cost function that is widely used is the **Sum of Squared Errors (SSE)** between the observed output and the predicted output. This function takes the form of <br>\n",
    "$J(\\mathbf{w}) = \\frac{1}{2} \\sum_{i}{\\left( y^{(i)} - \\phi \\left(z^{(i)} \\right) \\right)^{2}}$\n",
    "\n",
    "## Minimization of the objective function using gradient descent\n",
    "A simple, yet powerful algorithm for minimization of the cost function is **gradient descent** algorithm. Here, we aim to find the weights that minimize the objective function $J(\\mathbf{w})$. *The \"optimality\" of this method comes from the fact that the gradient of a scalar field creates the vector that points in the direction of the steepest ascent of the function - try to prove this)*. <br>\n",
    "\n",
    "The weights are updated as follows: <br>\n",
    "$\\mathbf{w} := \\mathbf{w} + \\Delta \\mathbf{w}$ \n",
    "<br>\n",
    "$\\Delta \\mathbf{w} = - \\eta \\nabla J(\\mathbf{w})$ <br>\n",
    "\n",
    "The component of the gradient of the cost function in the \"direction\" of each weight can then be written as <br> \n",
    "$\\frac{\\partial{J}}{\\partial{w_{j}}} = - \\sum_{i}{\\left( y^{(i)} - \\phi \\left(z^{(i)} \\right)  \\right) x^{(i)}_{j}}$\n",
    "<br> <br>\n",
    "Hence the weight update can be written as \n",
    "$\\Delta w_{j} = - \\eta \\sum_{i}{\\left( y^{(i)} - \\phi \\left(z^{(i)} \\right)  \\right) x^{(i)}_{j}}$\n",
    "<br> <br>\n",
    "The weights are updated simultaneuosly, based on all samples in the training set, and this is why the approach is also sometimes called **batch gradient descent**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of the Adaline in Python\n",
    "import numpy as np\n",
    "\n",
    "class AdalineGD(object):\n",
    "    \"\"\"ADaptive LInear NEuron classifier.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    eta : float\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "        Passes over the training dataset,\n",
    "    random_state : int\n",
    "        Random number generator seed for random weight initialization.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    w_ : 1d-array\n",
    "        Weights after fitting.\n",
    "    cost_ : list\n",
    "        Sum-of-squares cost function value in each epoch.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = {n_samples, n_features}\n",
    "        Training vectors, where n_samples is the number of samples and \n",
    "        n_features is the number of features.\n",
    "        y : array-like, shape = {n_samples}\n",
    "        \n",
    "            Target values.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        self : object\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size =1+X.shape[1])\n",
    "        \n",
    "        self.cost_ = []\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            net_input = self.net_input(X)\n",
    "            output = self.activation(net_input)\n",
    "            errors = (y - output)\n",
    "            self.w_[1:] += self.eta * X.T.dot(errors)\n",
    "            self.w_[0] += self.eta * errors.sum()\n",
    "            cost = (errors**2).sum() / 2.0\n",
    "            self.cost_ = append(cost)\n",
    "        return self\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "    \n",
    "    def activation(self, X):\n",
    "        \"\"\"Compute linear activation\"\"\"\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
